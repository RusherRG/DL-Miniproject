{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8240d-3d4c-4fb4-8356-9ff77808d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc68b1-e4df-4408-a9c1-063fbdcf9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    self.expansion * planes,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "class _ModifiedResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(_ModifiedResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(1024 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "    \n",
    "def ModifiedResNet():\n",
    "    return ResNet(BasicBlock, [1, 1, 1, 1])\n",
    "\n",
    "def ModifiedResNet_1():\n",
    "    return _ModifiedResNet(BasicBlock, [1, 1, 1])\n",
    "\n",
    "def ModifiedResNet_2():\n",
    "    return _ModifiedResNet(BasicBlock, [2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be14b5b-050d-4db6-bdea-5152cd00347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18().cuda()\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619bdd6-284d-4bc4-8c4e-f7fe15631de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedResNet().cuda()\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a7a1a-4bef-4a38-aa92-b472ae91bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedResNet_1().cuda()\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da611d90-29de-4d30-9723-17e87a443913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedResNet_2().cuda()\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cc194-553e-4cfa-bfb5-c57a6f8d1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(data_path, num_workers):\n",
    "    # reference: https://github.com/kuangliu/pytorch-cifar/blob/master/main.py#L30\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path, train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=128, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    transform_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path, train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=128, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def create_optimizer(model, optimizer_name, lr=0.1, momentum=0.9, weight_decay=5e-4):\n",
    "    if optimizer_name == \"sgd\":\n",
    "        return optim.SGD(\n",
    "            model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n",
    "        )\n",
    "    elif optimizer_name == \"sgd-nesterov\":\n",
    "        return optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=weight_decay,\n",
    "            nesterov=True,\n",
    "        )\n",
    "    elif optimizer_name == \"adam\":\n",
    "        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"adagrad\":\n",
    "        return optim.Adagrad(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"adadelta\":\n",
    "        return optim.Adadelta(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return None\n",
    "\n",
    "\n",
    "# training the model\n",
    "def train(model, train_loader, run_train, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        if run_train:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            num_batches = batch_idx\n",
    "\n",
    "    train_loss = train_loss / (num_batches + 1)\n",
    "    train_accuracy = 100.0 * (correct / total)\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, optimizer, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            num_batches = batch_idx\n",
    "\n",
    "    test_loss = test_loss / (num_batches + 1)\n",
    "    test_accuracy = 100.0 * (correct / total)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358e12d-4496-4043-9e22-d7e150e1a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    model,\n",
    "    use_cuda,\n",
    "    data_path,\n",
    "    run_train,\n",
    "    num_data_loader_workers,\n",
    "    num_epochs,\n",
    "    optimizer,\n",
    "    lr,\n",
    "    weight_decay=5e-4,\n",
    "    momentum=0.9,\n",
    "):\n",
    "    # creating train/test data loaders\n",
    "    train_loader, test_loader = create_data_loaders(data_path, num_data_loader_workers)\n",
    "\n",
    "    # check if cuda is available and can be used\n",
    "    # configure model to use cuda device accordingly\n",
    "    device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    if device == \"cuda\":\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = create_optimizer(model, optimizer, lr, momentum, weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(\n",
    "            model, train_loader, run_train, criterion, optimizer, device\n",
    "        )\n",
    "        test_loss, test_accuracy = test(\n",
    "            model, test_loader, criterion, optimizer, device\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} | [Train] Loss: {train_loss:.2f}, Acccuracy: {train_accuracy:.2f} | [Test] Loss: {test_loss:.2f}, Acccuracy: {test_accuracy:.2f}\"\n",
    "        )\n",
    "        if wandb.run is not None:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"train_accuracy\": train_accuracy,\n",
    "                    \"test_loss\": test_loss,\n",
    "                    \"test_accuracy\": test_accuracy,\n",
    "                    \"epoch\": epoch + 1,\n",
    "                }\n",
    "            )\n",
    "        if run_train:\n",
    "            scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38c8c5-f514-46b8-aa74-ba179b7ea5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_parameters(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        model = ModifiedResNet()\n",
    "        run(\n",
    "            use_cuda=True,\n",
    "            data_path=\"data/\",\n",
    "            run_train=True,\n",
    "            num_data_loader_workers=4,\n",
    "            num_epochs=25,\n",
    "            optimizer=config.optimizer,\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            momentum=config.momentum,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2fe2d8-d9e5-415d-a0c6-1f6d4f616169",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_config = {\n",
    "    \"name\": \"test_accuracy\",\n",
    "    \"goal\": \"maximize\",\n",
    "}\n",
    "\n",
    "parameters_config = {\n",
    "    \"optimizer\": {\n",
    "        \"values\": [\"adam\", \"sgd\", \"sgd-nesterov\", \"adadelta\", \"adagrad\"],\n",
    "    },\n",
    "    \"learning_rate\": {\"values\": [0.001, 0.005, 0.01, 0.05, 0.1]},\n",
    "    \"weight_decay\": {\"values\": [0.0001, 0.0005, 0.001, 0.005]},\n",
    "    \"momentum\": {\"values\": [0.9, 0.99]},\n",
    "}\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": metric_config,\n",
    "    \"parameters\": parameters_config,\n",
    "}\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fec55-e596-4cac-b546-bfb6507b8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"resnet-cifar\")\n",
    "# wandb.agent(sweep_id, tune_parameters, entity=\"aincrad\", project=\"resnet-cifar\", count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebbc20-e36d-4c60-a550-d7db955887e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66236d1d-e572-4e0b-ae53-f75206e3ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=\"ModifiedResNet\", project=\"resnet-cifar\", entity=\"rgg9776\")\n",
    "model = run(\n",
    "    model=ModifiedResNet(),\n",
    "    use_cuda=True,\n",
    "    data_path=\"data/\",\n",
    "    run_train=True,\n",
    "    num_data_loader_workers=8,\n",
    "    num_epochs=200,\n",
    "    optimizer=\"adagrad\",\n",
    "    lr=0.1,\n",
    "    weight_decay=5e-4,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff91b1-fa0b-4827-af97-8545504d6d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=\"ModifiedResNet-1\", project=\"resnet-cifar\", entity=\"rgg9776\")\n",
    "model = run(\n",
    "    model=ModifiedResNet_1(),\n",
    "    use_cuda=True,\n",
    "    data_path=\"data/\",\n",
    "    run_train=True,\n",
    "    num_data_loader_workers=8,\n",
    "    num_epochs=200,\n",
    "    optimizer=\"adagrad\",\n",
    "    lr=0.1,\n",
    "    weight_decay=5e-4,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a496a9-5e37-4320-8747-dfe67d65e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=\"ModifiedResNet-2\", project=\"resnet-cifar\", entity=\"rgg9776\")\n",
    "model = run(\n",
    "    model=ModifiedResNet_2(),\n",
    "    use_cuda=True,\n",
    "    data_path=\"data/\",\n",
    "    run_train=True,\n",
    "    num_data_loader_workers=8,\n",
    "    num_epochs=200,\n",
    "    optimizer=\"adagrad\",\n",
    "    lr=0.1,\n",
    "    weight_decay=5e-4,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
